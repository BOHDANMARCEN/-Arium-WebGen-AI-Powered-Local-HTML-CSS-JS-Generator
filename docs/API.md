# üì° API –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è

–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è API endpoints –¥–ª—è Arium WebGen.

## üìã –ó–º—ñ—Å—Ç

- [Generate Code](#generate-code)
- [Get Models](#get-models)
- [Get Default Provider](#get-default-provider)

## üîß Endpoints

### Generate Code

–ì–µ–Ω–µ—Ä—É—î HTML/CSS/JS –∫–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å—É.

**Endpoint:** `POST /api/generate-code`

**Request Body:**

```typescript
{
  prompt: string;                    // –û–ø–∏—Å –≤–µ–±-—Å—Ç–æ—Ä—ñ–Ω–∫–∏
  model: string;                     // –ù–∞–∑–≤–∞ –º–æ–¥–µ–ª—ñ
  provider: string;                   // –ü—Ä–æ–≤–∞–π–¥–µ—Ä (ollama, lm_studio, deepseek, openai_compatible)
  maxTokens?: number;                 // –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
  customSystemPrompt?: string;        // –ö–∞—Å—Ç–æ–º–Ω–∏–π —Å–∏—Å—Ç–µ–º–Ω–∏–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
}
```

**Response:**

Streaming response –∑ HTML –∫–æ–¥–æ–º.

**–ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:**

```typescript
const response = await fetch('/api/generate-code', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    prompt: '–°—Ç–≤–æ—Ä–∏ –ª–µ–Ω–¥—ñ–Ω–≥ –¥–ª—è –º–æ–±—ñ–ª—å–Ω–æ–≥–æ –∑–∞—Å—Ç–æ—Å—É–Ω–∫—É —É —Ñ—ñ–æ–ª–µ—Ç–æ–≤–∏—Ö —Ç–æ–Ω–∞—Ö',
    model: 'mistral',
    provider: 'ollama',
    maxTokens: 4000,
  }),
});

const reader = response.body?.getReader();
// –û–±—Ä–æ–±–∫–∞ streaming –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
```

**Thinking Models:**

–î–ª—è thinking –º–æ–¥–µ–ª–µ–π (Qwen, DeepCoder) –≤—ñ–¥–ø–æ–≤—ñ–¥—å –º—ñ—Å—Ç–∏—Ç—å –±–ª–æ–∫ –º—ñ—Ä–∫—É–≤–∞–Ω—å:

```html
<think>
  [–ü—Ä–æ—Ü–µ—Å –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è AI]
</think>
<!DOCTYPE html>
...
```

### Get Models

–û—Ç—Ä–∏–º—É—î —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤–∏–±—Ä–∞–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.

**Endpoint:** `GET /api/get-models?provider={provider}`

**Query Parameters:**

- `provider` (required): –ü—Ä–æ–≤–∞–π–¥–µ—Ä (ollama, lm_studio, deepseek, openai_compatible)

**Response:**

```typescript
{
  models: string[];  // –ú–∞—Å–∏–≤ –Ω–∞–∑–≤ –º–æ–¥–µ–ª–µ–π
}
```

**–ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:**

```typescript
const response = await fetch('/api/get-models?provider=ollama');
const data = await response.json();
console.log(data.models); // ['llama2', 'mistral', 'codellama', ...]
```

**–ü–æ–º–∏–ª–∫–∏:**

- `400 Bad Request` - –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
- `500 Internal Server Error` - –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞

### Get Default Provider

–û—Ç—Ä–∏–º—É—î –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó.

**Endpoint:** `GET /api/get-default-provider`

**Response:**

```typescript
{
  provider: string;  // –ù–∞–∑–≤–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
}
```

**–ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:**

```typescript
const response = await fetch('/api/get-default-provider');
const data = await response.json();
console.log(data.provider); // 'lm_studio'
```

## üîê –ê—É—Ç–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—è

API endpoints –Ω–µ –≤–∏–º–∞–≥–∞—é—Ç—å –∞—É—Ç–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—ó, –∞–ª–µ –¥–µ—è–∫—ñ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏ –ø–æ—Ç—Ä–µ–±—É—é—Ç—å API –∫–ª—é—á—ñ–≤, —è–∫—ñ –Ω–∞–ª–∞—à—Ç–æ–≤—É—é—Ç—å—Å—è —á–µ—Ä–µ–∑ –∑–º—ñ–Ω–Ω—ñ –æ—Ç–æ—á–µ–Ω–Ω—è.

## ‚ö†Ô∏è –û–±–º–µ–∂–µ–Ω–Ω—è

### Rate Limiting

- –õ–æ–∫–∞–ª—å–Ω—ñ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏ (Ollama, LM Studio): –ë–µ–∑ –æ–±–º–µ–∂–µ–Ω—å
- –•–º–∞—Ä–Ω—ñ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏: –ó–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞

### Token Limits

- –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∏–π –º–∞–∫—Å–∏–º—É–º: 8000 —Ç–æ–∫–µ–Ω—ñ–≤
- –ú—ñ–Ω—ñ–º—É–º: 100 —Ç–æ–∫–µ–Ω—ñ–≤

### Timeout

- –õ–æ–∫–∞–ª—å–Ω—ñ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏: 5 —Ö–≤–∏–ª–∏–Ω
- –•–º–∞—Ä–Ω—ñ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏: 2 —Ö–≤–∏–ª–∏–Ω–∏

## üêõ –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫

### –§–æ—Ä–º–∞—Ç –ø–æ–º–∏–ª–∫–∏

```typescript
{
  error: string;  // –û–ø–∏—Å –ø–æ–º–∏–ª–∫–∏
}
```

### –¢–∏–ø–æ–≤—ñ –ø–æ–º–∏–ª–∫–∏

**400 Bad Request:**
```json
{
  "error": "Missing required parameter: prompt"
}
```

**500 Internal Server Error:**
```json
{
  "error": "Cannot connect to Ollama. Is the server running?"
}
```

## üìù –ü—Ä–∏–∫–ª–∞–¥–∏

### –ü–æ–≤–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–æ–¥—É

```typescript
async function generateWebsite(description: string) {
  try {
    const response = await fetch('/api/generate-code', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        prompt: description,
        model: 'mistral',
        provider: 'ollama',
        maxTokens: 4000,
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error);
    }

    const reader = response.body?.getReader();
    let code = '';

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      const chunk = new TextDecoder().decode(value);
      code += chunk;
    }

    return code;
  } catch (error) {
    console.error('Error generating code:', error);
    throw error;
  }
}
```

### –û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π

```typescript
async function getAvailableModels(provider: string) {
  try {
    const response = await fetch(`/api/get-models?provider=${provider}`);
    
    if (!response.ok) {
      throw new Error('Failed to fetch models');
    }

    const data = await response.json();
    return data.models;
  } catch (error) {
    console.error('Error fetching models:', error);
    return [];
  }
}
```

## üîÑ WebSocket (–º–∞–π–±—É—Ç–Ω—î)

–ü–ª–∞–Ω—É—î—Ç—å—Å—è –¥–æ–¥–∞—Ç–∏ WebSocket –ø—ñ–¥—Ç—Ä–∏–º–∫—É –¥–ª—è real-time –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–∞ –æ–Ω–æ–≤–ª–µ–Ω—å.

## üìö –î–æ–¥–∞—Ç–∫–æ–≤—ñ —Ä–µ—Å—É—Ä—Å–∏

- [Next.js API Routes](https://nextjs.org/docs/app/building-your-application/routing/route-handlers)
- [OpenAI Streaming](https://platform.openai.com/docs/api-reference/streaming)

